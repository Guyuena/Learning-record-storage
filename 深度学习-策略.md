

# 2022-02-28

# 强化学习

![强化学习——马尔科夫决策过程和贝尔曼方程](https://pic1.zhimg.com/v2-2cdd0fc00282d3041b091c334e0e7015_1440w.jpg?source=172ae18b)





- **对于Agent：**主要涉及到三个组成要素：策略（Policy），价值函数（Value Function）和模型（Model），但要注意这三要素不一定要同时具备。

- - Policy：是Agent的**行为指南**，是一个从状态（s）到行动（a）的映射，可以分为确定性策略（Deterministic policy）和随机性策略（Stochastic policy），前者是指在某一特定状态确定对应着某一个行为a = π（s），后者是指在某一状态下，对应不同行动有不同的概率，即π（a|s）=P[At = a | St = s ]，可以根据实际情况来决定具体采用哪种策略。
  - Value Function：**价值函数是对未来总Reward的一个预测**，即如果我进入这个状态或者我采取这个行动的话能有多大的甜头或者风险。继而在做了计算以后选择更好的action。
  - Model：模型是指Agent**通过对环境状态的个人解读所构建出来的一个认知框架，它可以用来预测环境接下来会有什么表现**，比如，如果我采取某个特定行动那么下一个状态是什么，亦或是如果这样做所获得的奖励是多少。不过模型这个东西有些情况下是没有的。所以这就可以将Agent在连续决策(sequential decision making )行动中所遇到的问题划分为两种，即Learning problem 和 Planning problem。对于前者，没有环境的模型，Agent 只能通过和环境来互动来逐步提升它的策略。对于后者，环境模型已经有了，所以你怎么走会产生什么样的结果都是确定的了，这时候只要通过模型来计算那种行动最好从而提升自己策略就好。
  - 有关Agent的分类，从采取的方法上可以分为Value Based，Policy Based 和Actor
    Critic。第一种顾名思义就是基于价值函数的探索方式，第二种就是基于策略的探索方式，第三种就是前二者合体。另外，从是否含有模型上Agent又可分为Model Free 和Model Based。

- **对于Reward**：首先它是一个标量，是一个好坏的度量指标，然后Agent 的终极目标就是尽可能的最大化整个过程的累计奖励（cumulative reward），所以很多时候要把目光放长远一点，不要捡个芝麻丢个西瓜，要明白曲线救国也不是不可以的。



几个概念，即Exploration 探索 and Exploitation 开发，Prediction 预测 and Control 控制。因为强化学习是一个不断试错的过程，所以它肯定存在一个中间的动态平衡状态，是要继续去探索呢（Exploration）还是就此收手就用现在发现的最好（Exploitation），因为你现在发现的最好的方法可能只是个局部最优，所以探索还是使用？这真是一个问题。后边的那个prediction是指**对已知的策略进行评估**（evaluation），然后control就是对策略进行优化从而最终找到那个理论上最优的策略。



# 强化学习——马尔科夫决策过程和贝尔曼方程



马尔科夫过程（Markov Process，MP）
马尔科夫决策过程（Markov Decision Process，MDP）
值函数（Value Function）和动作值函数（action-value function）
贝尔曼方程（Bellman Equation）
贝尔曼最优方程（Bellman Optimality Equation）

## **Agent与Environment的交互**

![img](https://pic4.zhimg.com/80/v2-1f6b4f25b09962e76af16278179ced4b_1440w.jpg)

Agent每完成一个action，就会从environment得到反馈——state和reward。总结起来说，action就是一个任务中agent做出的选择；state是做出选择的基础；reward是评价这个选择做的好不好的基础。



## **马尔可夫（Markov）相关概念**

马尔可夫（Markov）相关概念包括马尔可夫过程（Markov Process），马尔可夫奖赏过程（Markov Reward Process），马尔可夫决策过程（Markov Decision Process）等。我们说他们都是具有马尔可夫性质（Markov Property）的，然后MRP就是再加上奖赏过程，MDP就是再加上决策过程。那么什么是马尔可夫性质呢？我们上边也提到过，用一句话来说就是“The
future is independent of the past given the present” 即 “在现在情况已知的情况下，过去与将来是独立的”再通俗一点就是我们可以认为现在的这个状态已经包含了预测未来所有的有用的信息，一旦现在状态信息我们已获取，那么之前的那些信息我们都可以抛弃不用了。

**几个重要概念：**

- **Markov Process （Markov Chain）**：是一个包含了状态S和转移概率P的数组<S,P>

- - 状态转移矩阵（StateTransition Matrix）：它描述的是在MP中，从所有前一个状态s（矩阵的行）转移到所有下一个状态s’（矩阵的列）的概率。我们可以想象，在MP的其中一个状态s时，他可以有不同的概率转移到后续的状态上，且这些概率加到一起是等于1的，也就是在这个矩阵里边每一行的概率加起来为1，当然不能转移到的状态填入0就ok。

- **Markov Reward Process（MRP）**:是加入了瞬时奖励（Immediate reward）和γ（discount factor）的MP, 即数组<S,P,R,γ>。这里边强调是瞬时奖励，即从前一个状态s一进入下一个状态s’就能获得的奖励。

- - 和瞬时奖励相对应的自然是累积奖励（cumulative reward），它包含了瞬时的奖励和后续步骤的奖励（乘以γ），这里边又把它称为Return。有关γ，前面提到过，范围[0,1]，表示目短浅／长远，之所以使用γ可能是用来弥补我们对环境不完善的建模或者因为其数学表达的便利性。但是一般情况下动物反射活动或者金融投资什么的都是偏向于瞬时奖励的。而如果我们知道有限的MP过程，即在你一眼可以望到边的情况下，是可以把γ设为１的。具体怎么平衡γ的取值可能课程后边会有涉及吧。
  - 状态价值函数v(s)：某一状态ｓ的价值函数，是指在MRP里边，从状态ｓ开始所期望获得的Return，所以根据定义，可以**将v(s)分为两部分，即瞬时奖励＋∑（后续各状态的瞬时奖励＊γ对应的次方），经过推倒可以转化为：用进入该状态的瞬时奖励＋（从该状态出发转移到下一状态的概率＊下一状态的价值函数＊γ）**。其实这也就是贝尔曼方程（Bellman Equation）的推导过程和核心思想。就是相当于建立起了连续两个状态之间价值函数的联系。这里注意下一个状态可能不止一个，所以就要分别算进来乘以他们各自的转移概率。而有人会问这里边“下一个状态”的价值函数怎么去求？还是用相同的方法，一步一步去套，相当于从“结束状态”开始倒推过来。DS课程里举了Student MDP的例子，大家可以尝试去计算。
  - 贝尔曼方程还有矩阵形式，就是把多个状态和对应的概率转移矩阵换了进来。那么如何解这个方程呢？因为Bellman Equation本来就是线性的，对于简单的MRP可以直接通过矩阵的转置来求，不过这里的推倒好像是认为当前状态的价值函数和下一状态的价值函数一致，所以就提取出来了。感觉不太符合常理，估计仅适合非常简单的模型吧。而复杂一点的就不能这样直接算了，智能通过迭代方法（iterative method）如动态规划，蒙特卡洛评估等方法。

- **Markov Decision Process（MDP）**：是加入了决策（Decision/Action）的MRP过程，所以包含<S,A,P,R,γ>。MRP只是陈述现实状态，并没有Agent参与采取行动，而MDP就有Agent过来指手画脚了，毕竟我们的终极目标是想看哪种方法是能获取奖励最多的，最优决策。

- - Policy（策略，π）：我们第一部分也介绍了策略是什么，他就是agent的一个行动指南，**即在什么状态有多少概率去采取什么行动，是一个s到a的映射。所以策略是依赖于当时所处的状态的。**
  - 在策略π下的Value Function 【Vπ（s）】:顾名思义，之前是所有状态的价值函数，现在呢是在策略π之下的状态价值函数，即如果我使用策略π，那么我每个状态的价值函数会变成什么样呢？毕竟我们现在已经有个策略了，不cover在我们策略π里边的状态我们就不考虑了。还有个**更细化的定义**叫做Action-value Function【qπ（s，a）】行动价值函数，**是指从状态s开始如果遵从策略π并采用行动a的话，我能得到的return是多少**。之所以说**更细化是因为一个Vπ（s）里有多种s与a的映射关系**，而现在我仅仅采用其中一个具体的a看看到底怎么样。同样，具体涉及到计算方面我们还是用Bellman Equation的“两部走”分解思想。课程ppt里后边画树状图方法一步步做分解来作解释，可以慢慢体会，其实和前边一样。
  - 上边我们从状态价值函数【v(s)】具体到在策略π下的Value Function【Vπ（s）】，又具体到在策略π下采取行动a的行动价值函数【qπ（s，a）】，下面我们再递进一下，我们做这么多的终极目标是什么？我们要找最优策略！所以下面又出来了个[V*(s)]和【q*（s，a）】，**前者是在所有策略中最大的那个状态价值函数**，**后者,是在所有行动中行动价值函数最大的那个**。继而，又出现了**最优策略（optimal policy）的概念，顾名思义，就是在这么多种策略里边，能达到最大策略价值函数的那个**。同样，行动价值函数中最大的那个就是最优行动。在具体计算过程中，要注意有选择action的时候选max那个，而通过一个action转移到各个状态的时候就要算上各自的概率加权了。

David Silver在伦敦大学开的强化学习课程，B站搜就有带中文字幕的

   



## **马尔科夫过程（Markov Process，MP）**

我们说一个state若满足 ，则其具有马尔可夫性，即该state完全包含了历史中的所有信息。马尔科夫过程是无记忆的随机过程，即随机状态序列 具有马尔可夫属性。

 即 “在现在情况已知的情况下，过去与将来是独立的”再通俗一点就是我们可以认为现在的这个状态已经包含了预测未来所有的有用的信息，一旦现在状态信息我们已获取，那么之前的那些信息我们都可以抛弃不用了。

马尔可夫性质:

​	当一个[随机过程](https://baike.baidu.com/item/随机过程)在给定现在状态及所有过去状态情况下，其未来状态的条件[概率分布](https://baike.baidu.com/item/概率分布)仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此[随机过程](https://baike.baidu.com/item/随机过程)即具有**马尔可夫性质**。

马尔可夫过程：

​	具有马尔可夫性质的过程通常称之为**[马尔可夫过程](https://baike.baidu.com/item/马尔可夫过程)**。

一个马尔科夫过程可以由一个元组组成 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%3C+%5Cmathcal%7BS%7D%2C%5Cmathcal%7BP%7D+%5Cright%3E+)

- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BS%7D+) 为（有限）的状态（state）集；
- ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D+) 为**状态转移矩阵**， ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BP%7D_%7Bss%27%7D%3D%5Cmathbb%7BP%7D%5Cleft%5B+S_%7Bt%2B1%7D%3Ds%27%7CS_t%3Ds+%5Cright%5D+) 。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以矩阵每一行元素之和为1



## **马尔科夫决策过程（Markov Decision Process，MDP）**

MDP相对于MP加入了瞬时奖励 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BR%7D)(Immediate reward）、动作集合 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BA%7D+) 和折扣因子 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cgamma+) （Discount factor），这里的瞬时奖励说的是从一个状态 ![[公式]](https://www.zhihu.com/equation?tex=s) 到下一个状态 ![[公式]](https://www.zhihu.com/equation?tex=+s%27+) 即可获得的rewards，虽然是“奖励”，但如果这个状态的变化对实现目标不利，就是一个负值，变成了“惩罚”，所以reward就是我们告诉agent什么是我们想要得到的，但不是我们如何去得到。

MDP由元组 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%3C+%5Cmathcal%7BS%7D%2C%5Cmathcal%7BA%7D%2C%5Cmathcal%7BP%7D%2C%5Cmathcal%7BR%7D%2C%5Cgamma+%5Cright%3E+) 定义。其中

- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BS%7D+) 为（有限）的状态（state）集；
- ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BA%7D+) 为有限的动作集；
- ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D+) 为状态转移矩阵。所谓状态转移矩阵就是描述了一个状态到另一个状态发生的概率，所以矩阵每一行元素之和为1。![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%3D%5Cmathbb%7BP%7D%5Cleft%5B+S_%7Bt%2B1%7D%3Ds%27%7CS_t%3Ds%2CA_t%3Da+%5Cright%5D+)

![\mathbb P= \left|\begin{matrix}P_{11} & ...&P_{1n}\\\\. & &.\\\\P_{n1} &... &P_{nn}\end{matrix}\right|](https://math.jianshu.com/math?formula=%5Cmathbb%20P%3D%20%5Cleft%7C%5Cbegin%7Bmatrix%7DP_%7B11%7D%20%26%20...%26P_%7B1n%7D%5C%5C%5C%5C.%20%26%20%26.%5C%5C%5C%5CP_%7Bn1%7D%20%26...%20%26P_%7Bnn%7D%5Cend%7Bmatrix%7D%5Cright%7C)

- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BR%7D) 为回报函数（reward function), ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%3D%5Cmathbb%7BE%7D%5Cleft%5B+R_%7Bt%2B1%7D%7CS_t%3Ds%2CA_t%3Da+%5Cright%5D+)

- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cgamma+) 为折扣因子，范围在[0,1]之间， 越大，说明agent看得越“远”。

折扣因子通常以符号γ表示，在强化学习中用来调节近远期影响，即agent做决策时考虑多长远，取值范围(0,1]。γ越大agent往前考虑的步数越多，但训练难度也越高；γ越小agent越注重眼前利益，训练难度也越小。我们都希望agent能“深谋远虑”，但过高的折扣因子容易导致算法收敛困难。还以小车导航为例，由于只有到达终点时才有奖励，相比而言惩罚项则多很多，在训练初始阶段负反馈远多于正反馈，一个很高的折扣因子（如0.999）容易使agent过分忌惮前方的“荆棘丛生”，而宁愿待在原地不动；相对而言，一个较低的折扣因子（如0.9）则使agent更加敢于探索环境从而获取抵达终点的成功经验；而一个过低的折扣因子（如0.4），使得稍远一点的反馈都被淹没了，除非离终点很近，agent在大多数情况下根本看不到“光明的未来”，更谈不上为了抵达终点而努力了。



比如  0.2^10 < 0.9^10  也就是 γ=0.9 同样都是10步，但0.9 比0.5 更看重第十步的作用





## **值函数（Value Function）和动作值函数（action-value function）**

这里就不得不提一个概念——回报 ![[公式]](https://www.zhihu.com/equation?tex=+G_t+) （Return），回报 描述了从时间 ![[公式]](https://www.zhihu.com/equation?tex=t) 起的总折扣奖励，即

Gain 

![[公式]](https://www.zhihu.com/equation?tex=+G_t%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B...%3D%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma+%5EkR_%7Bt%2Bk%2B1%7D%7D+)

第一次状态S0 下做出动作 a1 后Agent得到的状态S1的G1  

第一次状态S1 下做出动作 a2 后Agent得到的状态S2的G2 

第一次状态St-1下做出动作 at 后Agent得到的状态St的Gt ,不断向前累计



一般来说，我们的任务分两种类型，一种是可以分为一个个episode的任务，我们称之为episodic tasks，这里的折扣因子一般取 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cgamma+%3D1+)另一种是连续的任务，与前一种相对应，我们称之为continuing tasks，这时折扣因子一般取0到1之间的一个数。



**我们的最终的目标就是让回报的期望（expected return）最大。**

我们如何去衡量一个agent转移到一个state对于我们达到目的有“多有用（how good）”呢？上面也提到了，那就是使回报的期望最大，而agent每一步得到的reward与其采取的action有关，所以值函数是在一个（一组）action下定义的，我们称之为策略（policy）。所以 MDP中一个状态s在策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+) 下的值函数记为 ![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) **，它代表从s开始的回报的期望**。

![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B+G_t%7CS_t%3Ds+%5Cright%5D+)

相似的，我们定义从状态s开始，**并执行动作a**，**然后遵循策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)** 所获得的回报的期望记为动作值函数（action-value function)

![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B+G_t%7CS_t%3Dt%2CA_t%3Da+%5Cright%5D+)

*这是两个概念，分别是状态价值V(s)和动作价值Q(s,a)，前者是对环境中某一个状态的价值大小做的评估，后者是对在某状态下的动作的价值大小的评估。概念类似，但主要区别应该是体现在用途以及算法上吧，比如对于离散型的动作空间，可以单纯基于动作Q值去寻优（DQN算法），如果是动作空间巨大或者动作是连续型的，那么可以判断状态价值并结合策略梯度来迭代优化（AC算法）～*

简单来说就是不同场景下我们会选择动作值函数或者状态值函数来寻找最优策略

**一个评估状态的好坏，一个评估具体状态下可能采取的动作的好坏**



在策略π下的Value Function 【Vπ(s)】:顾名思义，之前是所有状态的价值函数，现在呢是在策略π之下的状态价值函数，即如果我使用策略π，那么我每个状态的价值函数会变成什么样呢？毕竟我们现在已经有个策略了，不cover在我们策略π里边的状态我们就不考虑了。还有个更细化的定义叫做Action-value Function【qπ（s，a）**】行动价值函数，是指从状态s开始如果遵从策略π并采用行动a的话，我能得到的return是多少**。之所以说更细化是因为一个Vπ（s）里有多种s与a的映射关系，而现在我仅仅采用其中一个具体的a看看到底怎么样











## **贝尔曼方程（Bellman Equation）**

状态值函数可以分解为瞬时回报加上后续状态的折扣值，即

![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B+R_%7Bt%2B1%7D%2B%5Cgamma+v_%7B%5Cpi%7D%5Cleft%28+S_%7Bt%2B1%7D+%5Cright%29+%7CS_t%3Ds+%5Cright%5D+)





动作值函数也可以如此分解：

![[公式]](https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B+R_%7Bt%2B1%7D%2B%5Cgamma+q_%7B%5Cpi%7D%5Cleft%28+S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D+%5Cright%29+%7CS_t%3Ds%2CA_t%3Da+%5Cright%5D)

下面这幅图叫做backup diagrams，其中每一个空心圆圈代表一个状态（state），一个实心的圆圈代表一个状态-动作对（state-action pair）

![img](https://pic3.zhimg.com/80/v2-d4174815e22b7426867a915e363f3862_1440w.jpg)

​																						图2 backup diagrams

由图，可以得到如下关系

![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%3D%5Csum_%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%7Cs+%5Cright%29+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29%7D+)

​																					策略下的概率*策略下动作的价值

![[公式]](https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_%7B%5Cpi%7D%5Cleft%28+s%27+%5Cright%29)

于是有

![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%3D%5Csum_%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%7Cs+%5Cright%29+%5Cleft%28+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_%7B%5Cpi%7D%5Cleft%28+s%27+%5Cright%29+%5Cright%29%7D+)

![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7D%5Csum_%7Ba%27%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%27%7Cs%27+%5Cright%29+q_%7B%5Cpi%7D%5Cleft%28+s%27%2Ca%27+%5Cright%29%7D+)

上述两个式子分别称为关于 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 和 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+) 的贝尔曼方程，其描述了当前状态值函数和其后续状态值函数之间的关系，即状态值函数（动作值函数）**等于瞬时回报的期望加上下一状态的（折扣）状态值函数（动作值函数)的期望**。



## **贝尔曼最优方程（Bellman Optimality Equation）**

强化学习的目标是找到一个最优策略，使得回报最大。准确的说是使值函数最大，包括状态值函数和动作值函数，分别记为 ![[公式]](https://www.zhihu.com/equation?tex=v_%2A%5Cleft%28+s+%5Cright%29+) 和![[公式]](https://www.zhihu.com/equation?tex=+q_%2A%5Cleft%28+s%2Ca+%5Cright%29+)

![[公式]](https://www.zhihu.com/equation?tex=+v_%2A%5Cleft%28+s+%5Cright%29+%3D%5Cunderset%7B%5Cpi%7D%7B%5Cmax%5Ctext%7B%5C+%7D%7Dv_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+)

![[公式]](https://www.zhihu.com/equation?tex=q_%2A%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cunderset%7B%5Cpi%7D%7B%5Cmax%5Ctext%7B%5C+%7D%7Dq_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+)

对于任意一个MDPs，总是存在一个最优的策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+_%2A+) ，在使用这个策略时就能取得最优值函数，即 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi+_%2A%7D%5Cleft%28+s+%5Cright%29+%3Dv_%2A%5Cleft%28+s+%5Cright%29+) ， ![[公式]](https://www.zhihu.com/equation?tex=q_%7B%5Cpi+_%2A%7D%5Cleft%28+s%2Ca+%5Cright%29+%3Dq_%2A%5Cleft%28+s%2Ca+%5Cright%29+) ，于是我们可以得到贝尔曼最优方程

![[公式]](https://www.zhihu.com/equation?tex=v_%2A%5Cleft%28+s+%5Cright%29+%3D%5Cunderset%7Ba%7D%7B%5Cmax%7D%5C+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7Dv_%2A%5Cleft%28+s%27+%5Cright%29%7D+)

![[公式]](https://www.zhihu.com/equation?tex=q_%2A%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%5Cunderset%7Ba%27%7D%7B%5Cmax%7D%5C+q_%2A%5Cleft%28+s%27%2Ca%27+%5Cright%29%7D+)

上述贝尔曼最优方程是非线性的，不存在闭式解，常常使用一些迭代的方法求解，比如值迭代、策略迭代、Q-learning、Sarsa等







# 强化学习——MDPs求解之动态规划

策略评估（Policy Evaluation）
策略提升（Policy Improvement）
策略迭代（Policy Iteration）
值迭代（Value Iteration）

动态规划（Dynamic Programming, DP）是一种解决复杂问题的方法，它通过定义问题状态和状态之间的关系，将复杂问题拆分成若干较为简单的子问题，使得问题能够以递推（或者说分治）的方式去解决。**所以要能使用动态规划，这种问题一要能够分解成许多子问题，二要这些子问题能够多次被迭代使用。**而马尔科夫决策过程就正好满足了这两个条件，MDPs可以看成是各个状态之间的转移，而贝尔曼方程则将这个问题分解成了一个个状态的递归求解问题，而值函数就用于存储这个求解的结果，得到每一个状态的最优策略，合在一起以后就完成了整个MDPs的求解。但是DP的使用时建立在我们知道MDP环境的模型的基础上的，所以也称其为model based method。

## **策略评估（Policy Evaluation）**

**策略评估如其字面意思，就是评价一个策略好不好**。计算任意一个策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+) 的状态值函数 ![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 即可，这也叫做预测（Prediction），[上一篇文章](https://zhuanlan.zhihu.com/p/34021617)已经通过backup图得到了 的求解公式，如下：

![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%3D%5Csum_%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%7Cs+%5Cright%29+%5Cleft%28+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_%7B%5Cpi%7D%5Cleft%28+s%27+%5Cright%29+%5Cright%29%7D+)

那这个式子怎么算呢？状态 ![[公式]](https://www.zhihu.com/equation?tex=+s%27+) 的值函数我也不知道啊。这里我们会使用[高斯-赛德尔迭代](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF-%E8%B5%9B%E5%BE%B7%E5%B0%94%E8%BF%AD%E4%BB%A3)算法来求解，先人为给一个初值，再根据下面的式子迭代求解，可以证明，当k趋于无穷时，最后是会收敛到 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 的。

![[公式]](https://www.zhihu.com/equation?tex=v_%7Bk%2B1%7D%5Cleft%28+s+%5Cright%29+%3D%5Csum_%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%7Cs+%5Cright%29+%5Cleft%28+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_k%5Cleft%28+s%27+%5Cright%29+%5Cright%29%7D+)

<img src="C:\Users\jc\AppData\Roaming\Typora\typora-user-images\image-20220228170833650.png" alt="image-20220228170833650" style="zoom: 67%;" />

<img src="C:\Users\jc\AppData\Roaming\Typora\typora-user-images\image-20220228170901237.png" alt="image-20220228170901237" style="zoom:67%;" />

![policy-evalution.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/policy-evalution.png)

<img src="C:\Users\jc\AppData\Roaming\Typora\typora-user-images\image-20220228171041655.png" alt="image-20220228171041655" style="zoom: 67%;" />

## 策略评估案例分析：小小方格世界

如下图在4x4方格中，共有15种状态，其中灰色底色的方块表示终止状态（注意，此时图中的数字表示方块的序号，不是状态的价值）。游戏的规则是：

- agent在任意一个方块中共有4种动作{up,down,left,right}；
- 每移动一步的即时奖励为-1；显然，在终止状态的奖励为0。
- 如果动作导致agent超出了边界，则agent回到移动前的状态；

下面，我们评估一下随机游走的策略，**即agent在一个状态下采取4种动作的概率均为0.25。**首先，初始化所有的状态值函数为0（k=0），如下图所示：

![grid-world-0.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-0.png)

如何计算下一轮迭代的状态值函数呢？以1号状态为例，画出1号状态的backup diagram如下图所示：

![grid-world-status-1-step-1-0.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-status-1-step-1.png)

由此可以计算出1号状态的价值函数为：



v1(1)=0.25×q(1,up)+0.25×q(1,down)+0.25×q(1,left)+0.25×q(1,right)



从这个式子就知道为什么说，V()是Q()关于动作a的期望了

**式中，q(1,up)q(1,up)表示在状态1采取动作up的动作价值函数**。以计算q(1,up)q(1,up)为例：



q(1,up)=−1+1×0=−1



其中，-1是采取动作up的即时奖励；1是采取动作up时后续状态的转移概率。因为采取动作up后只有一个后续状态（超出边界回到状态1），因此转移概率为1；0是上一轮迭代时后续状态（这里依然是状态1）的值价值函数。

下面是计算1号状态价值函数在第一轮迭代（k=1）的完整过程：



v1(1)=0.25×(−1+1×0)+0.25×(−1+1×0)+0.25×(−1+1×0)+0.25×(−1+1×0)(4-1,up)(4-2,down)(4-3,left)(4-4,right)(1)(4-1,up)

v1(1)=0.25×(−1+1×0)+(4-2,down)0.25×(−1+1×0)+(4-3,left)0.25×(−1+1×0)+(4-4,right)0.25×(−1+1×0)(1)=−1

很容易看出，在第一轮迭代结束时，除终止状态外，其他所有状态的价值函数均为-1，如下图所示（k=1）：

![grid-world-1.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-1.png)

下面进行第二轮迭代，还是以状态1为例：



v2(1)==1.750.25×(−1+1×−1)+0.25×(−1+1×−1)+0.25×(−1+1×0)+0.25×(−1+1×−1)(4-1,up)(4-2,down)(4-3,left)(4-4,right)(2)(4-1,up)v2(1)=0.25×(−1+1×−1)+(4-2,down)0.25×(−1+1×−1)+(4-3,left)0.25×(−1+1×0)+(4-4,right)0.25×(−1+1×−1)(2)=1.75

第二轮迭代结束时，状态如下图所示（k=2）：

![grid-world-2.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-3.png)

可以看出，离终止状态较近的状态其值函数更大一些，表示从这些状态到终止状态所需的步数更少。

后面轮次的计算过程不再赘述，下面分别是第三轮迭代后的状态（k=3）：

![grid-world-3.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-4.png)

第十轮迭代后的状态（k=10）：

![grid-world-11.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-11.png)

当k=∞k=∞时，状态为：

![grid-world-infinity.png](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/rl/dp/grid-world-infinity.png)



公式： ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bk%2B1%7D%3D%5Csum_%7Ba%5Cin+A%7D%7B%5Cpi%28a%7Cs%29%7D%28R_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs_%7B%27%7D%5Cin+s%7D%7BP_%7Bss_%7B%27%7D%7D%5E%7Ba%7D%7Dv_%7Bk%7D%28s%5E%7B%27%7D%29%29)





[强化学习(四)--动态规划方法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/39699379)

**举一个例子：**在下面的网格里，其状态空间为 ![[公式]](https://www.zhihu.com/equation?tex=S%3D) {1，2，3，...，14}，动作空间 ![[公式]](https://www.zhihu.com/equation?tex=A%3D) {东，南，西，北}。奖励函数 ![[公式]](https://www.zhihu.com/equation?tex=r%3D-1) ，折扣系数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%3D1) ，需要评估的**策略为均匀随机策略**， ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%28%E4%B8%9C%7C%5Cbullet%29%3D0.25) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%28%E5%8D%97%7C%5Cbullet%29%3D0.25) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%28%E8%A5%BF%7C%5Cbullet%29%3D0.25) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%28%E5%8C%97%7C%5Cbullet%29%3D0.25) 。

![img](https://pic4.zhimg.com/80/v2-611bb058bdf3d0d43aaca17144c729fb_1440w.jpg)David Silver的策略评估例子

下面介绍迭代过程。

**当k=0时，**状态值函数都为0；

![img](https://pic4.zhimg.com/80/v2-3be8317ed6a7aa4ca599c9147a8182c3_1440w.jpg)

**当k=1时，**

![img](https://pic3.zhimg.com/80/v2-ed359f6f23614bd9f3d2c302b1e43242_1440w.jpg)

当Agent走到位置1处，东南西北四个方向的状态值函数都是0。

![[公式]](https://www.zhihu.com/equation?tex=v_%7B1%7D%281%29%3D0.25%5Ctimes%28-1%2B1%5Ctimes0%29%2B0.25%5Ctimes%28-1%2B1%5Ctimes0%29%2B0.25%5Ctimes%28-1%2B1%5Ctimes0%29%2B0.25%5Ctimes%28-1%2B1%5Ctimes0%29+%3D-1.0)

其中，0.25是策略π，-1是当前状态的奖励，1是折扣系数 ，0是下一状态的值函数。

![img](https://pic4.zhimg.com/80/v2-504eadb6aa1b75fd8e3ecbd4aabf0f83_1440w.jpg)

**当k=2时，**

![img](https://pic4.zhimg.com/80/v2-201c88917d8dfa91c26fd10752f39927_1440w.jpg)

当Agent走到位置1时，东南西北四个方向的状态值函数已经不一样了。

![[公式]](https://www.zhihu.com/equation?tex=v_%7B2%7D%281%29%3D0.25%5Ctimes%28-1-1%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes0%29%2B0.25%5Ctimes%28-1-1%5Ctimes1%29%3D-1.75)

![[公式]](https://www.zhihu.com/equation?tex=v_%7B2%7D%282%29%3D0.25%5Ctimes%28-1-1%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes1%29%3D-2.0)

![img](https://pic2.zhimg.com/80/v2-bc9a5f36cc19f4a94691e706017c14a1_1440w.jpg)

**当k=3时，**

![img](https://pic1.zhimg.com/80/v2-b0a1451af9c665abda731d739fd37d5c_1440w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=v_%7B3%7D%281%29%3D0.25%5Ctimes%28-1-1.7%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-1%5Ctimes0%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%3D-2.425)

![[公式]](https://www.zhihu.com/equation?tex=v_%7B3%7D%282%29%3D0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-1.7%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%3D-2.925)

![[公式]](https://www.zhihu.com/equation?tex=v_%7B3%7D%282%29%3D0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%2B0.25%5Ctimes%28-1-2.0%5Ctimes1%29%3D-3.0)



**当k=10时，**

![img](https://pic4.zhimg.com/80/v2-dc28b3fd24beb92a31bceb8d0b2d7553_1440w.jpg)

策略评估迭代10次和无穷次所得到的贪婪策略是一样的。











## **策略提升（Policy Improvement）**

我们已经知道怎么去评价一个策略好不好，那接下来就要找到那个最好的策略。每到一个状态，我们可能就会想是不是需要改变一下策略，这样也许能使回报更大，即选择一个动作 ![[公式]](https://www.zhihu.com/equation?tex=a%5Cne+%5Cpi+%5Cleft%28+s+%5Cright%29+) ，然后再继续遵循 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+) ，这种方式的值就是动作值函数

![[公式]](https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_%7B%5Cpi%7D%5Cleft%28+s%27+%5Cright%29+)

我们用一种贪婪的方式来提升我们策略，即选择那个能使动作值函数最大的动作：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+%27%5Cleft%28+s+%5Cright%29+%3D%5Cunderset%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7Barg%5Cmax%7Dq_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+)

可以证明，改变了策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+) 以后，状态值函数也变大了，即 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi+%27%7D%5Cleft%28+s+%5Cright%29+%5Cgeqslant+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+)



## **策略迭代（Policy Iteration）**

说完了策略评估和策略提升，策略迭代就简单了，就是反复使用策略评估和策略提升，最后会收敛到最优策略。

<img src="https://pic4.zhimg.com/80/v2-cd2e3167e43b5226fa580d0ed70368c3_1440w.jpg" alt="img" style="zoom:67%;" />

<img src="https://pic3.zhimg.com/80/v2-52d68a9f20ef15a42ab5911c19df748e_1440w.jpg" alt="img" style="zoom:67%;" />

其伪代码如图所示

![img](https://pic3.zhimg.com/80/v2-b071cf4b1637cbaa8a7ae658552f527e_1440w.jpg)

## **值迭代（Value Iteration）**

策略迭代有一个缺点，就是每一步都要进行策略评估，当状态空间很大的时候是非常耗费时间的。值迭代是直接将贝尔曼最优化方程拿来迭代计算的，这一点是不同于策略迭代的，我们直接对比两者的伪代码。

![img](https://pic2.zhimg.com/80/v2-5e937257be57bee901319da51867c789_1440w.jpg)

所以值迭代会直接收敛到最优值，从而我们就可以得到最优策略，因为它就是一个贪婪的选择。再反过去看一下策略迭代的过程，策略评估过程是应用贝尔曼方程来计算当前最优策略下的值函数，接着进行策略提升，即在每个状态都选择一个最优动作来最大化值函数，以改进策略。但是想一下，在策略评估过程我们一定要等到它收敛到准确的值函数吗？答案是不一定，我们可以设定一个误差，中断这个过程，用一个近似的值函数用以策略提升

[策略迭代与值迭代的区别_庞琳卓的博客-CSDN博客_策略迭代和值迭代](https://blog.csdn.net/panglinzhuo/article/details/77752574)















# 强化学习——蒙特卡洛方法





## Prediction和Control



预测（Prediction）和控制（Control）是MDP（Markov decision process）中的两类问题：

S：状态

A： 动作

P：转移概率

R：回报，奖励

γ： 折扣回报率

**预测问题：**

- 输入：MDP ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%3C+%5Cmathcal%7BS%7D%2C%5Cmathcal%7BA%7D%2C%5Cmathcal%7BP%7D%2C%5Cmathcal%7BR%7D%2C%5Cgamma+%5Cright%3E+) 和策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+)
- 输出：状态值函数 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D+) 或者状态动作值函数 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D)

**控制问题**

- 输入：MDP ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%3C+%5Cmathcal%7BS%7D%2C%5Cmathcal%7BA%7D%2C%5Cmathcal%7BP%7D%2C%5Cmathcal%7BR%7D%2C%5Cgamma+%5Cright%3E+)
- 输出：最优状态值函数 ![[公式]](https://www.zhihu.com/equation?tex=v_%2A+) 或者最优状态动作值函数 ![[公式]](https://www.zhihu.com/equation?tex=q_%2A+) ，和最优策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+_%2A+)

动态规划方法，两者的对应关系如下图：

![img](https://pic4.zhimg.com/80/v2-bcdb04a2a330ec1c1d9185e0261f767f_1440w.jpg)



| 问题 | 贝尔曼方程                  | 算法         |
| ---- | --------------------------- | ------------ |
| 预测 | 贝尔曼期望方程              | 迭代策略评估 |
| 控制 | 贝尔曼期望方程+贪婪策略改进 | 策略迭代     |
| 控制 | 贝尔曼最优方程              | 值迭代       |

## **蒙特卡洛方法简述**

动态规划方法是建立在模型已知的情况下，但是往往大多数情况下模型是未知的，实际应用中我们不可能完全了解一个环境的所有知识，比如说得出它的状态转移矩阵。这个时候蒙特卡洛算法就派上用场了，它只需要从经验（experience）中去学习，这个经验包括样本序列的状态（state）、动作（action）和奖励（reward）。得到若干样本的经验后，通过**平均所有样本的回报（return）**来解决强化学习的任务。

一个episode就可以看作是一个样本，假设对于状态 ![[公式]](https://www.zhihu.com/equation?tex=+s+) ，给定策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) ，要计算其值函数 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 。在一个episode中，**每次状态出现都称为一次visit**，当然在一个episode中， ![[公式]](https://www.zhihu.com/equation?tex=s) 可能出现多次。我们称**第一次出现该状态为first-visit**，因此first-visit蒙特卡洛方法（first-visit MC method）就是将所有**第一次**访问到 ![[公式]](https://www.zhihu.com/equation?tex=s) 得到的回报求均值。根据大数定理，当样本足够大的时候，该均值就趋近于 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 。顾名思义，every-visit蒙特卡洛方法（first-visit MC method）就是将**所有**访问到 ![[公式]](https://www.zhihu.com/equation?tex=s) 得到的回报求均值。下面的算法就是估计 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D+) 的first-visit MC方法：

![img](https://pic2.zhimg.com/80/v2-9d40115ee267a40e839a1095bc921aa1_1440w.jpg)

说了这么多，估计状态值函数对于我们有用吗？回想一下DP方法中我们是怎么计算 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) 的：

![[公式]](https://www.zhihu.com/equation?tex=v_%7Bk%2B1%7D%5Cleft%28+s+%5Cright%29+%3D%5Csum_%7Ba%5Cin+%5Cmathcal%7BA%7D%7D%7B%5Cpi+%5Cleft%28+a%7Cs+%5Cright%29+%5Cleft%28+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma+%5Csum_%7Bs%27%5Cin+%5Cmathcal%7BS%7D%7D%7B%5Cmathcal%7BP%7D_%7Bss%27%7D%5E%7Ba%7D%7Dv_k%5Cleft%28+s%27+%5Cright%29+%5Cright%29%7D+)

我们是通过one step look ahead的方法来迭代求解的。但是此时我们并不知道模型的具体情况，状态转移矩阵也不知道，所以这种求法就行不通了！还记得当时我在[强化学习——马尔科夫决策过程和贝尔曼方程](https://zhuanlan.zhihu.com/p/34021617)这篇文章中提的一个问题吗？为什么要有状态值函数（state value
function）和状态动作值函数（state-action
function）这两个概念，明明一个状态值函数就可以表征我们强化学习的目标——最大化回报，学到这里就可以给一个说法了，因为**在模型已知的时候，只用状态值函数就可以完全决定策略，而模型未知的时候就必须估计每个动作的值，即状态动作值函数。**所以，蒙特卡洛了方法就是去得到 ![[公式]](https://www.zhihu.com/equation?tex=+q_%2A+) ，则对应的策略估计问题就是估计 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+) ，即从状态 ![[公式]](https://www.zhihu.com/equation?tex=s) 开始，并采取动作 ![[公式]](https://www.zhihu.com/equation?tex=a) ，然后遵循策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) 得到的回报的期望。根据上面的讨论，**first-visit MC方法去估计** ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+) **就是求所有episode第一次访问** ![[公式]](https://www.zhihu.com/equation?tex=+%5Cleft%28+s%2Ca+%5Cright%29+) **这个state-action pair所得到回报的均值。**

## **蒙特卡洛方法的控制问题——策略提升**

根据广义的策略迭代算法，得到值函数以后，下一步就是进行提升，进而得到最优值函数和最优策略。

![img](https://pic1.zhimg.com/80/v2-0d59220ab19e5f921a3309ceacdab40c_1440w.jpg)

![img](https://pic4.zhimg.com/80/v2-299e466abdbf2ee8d906d860ced31493_1440w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+%5Cleft%28+s+%5Cright%29+%3D%5Cunderset%7Ba%7D%7Barg%5Cmax%7D%5C+q%5Cleft%28+s%2Ca+%5Cright%29+)

为了使这个过程收敛，我们是建立在两个假设上面的：

**1.** **策略估计过程需要无限个episode才会收敛到回报的期望；**

**2.** **Exploring starts，即能够保证状态集合** ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BS%7D+) **中的所有状态都是有可能被选中为每个episode的初始状态。**

显然，在实际的算法中这是不可能实现的，我们必须想法去掉这两个假设！先看第一个假设，其实这个假设我们在DP方法中也是遇到的，有两种方法可以去掉这个假设。

方法一：坚持在每次策略评估的过程中接近 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi+k%7D+) 。有点懵是不是？这是sutton书中的一句话，原话是“One is to hold firm to the idea of approximating ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi+k%7D+) in each policy evaluation.”其实很简单，就是说虽然理论上必须有无限个episode来作为样本去评估 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi+k%7D+) ，实际上是做不到的，我只能尽力多产生点episode，尽可能的去接近这个收敛值。我们可以设定一个误差，两次估计的值小于这个误差，差不多就行了。

<img src="https://pic4.zhimg.com/80/v2-c9991d0cb7e15acbc65c015e5cf774f7_1440w.jpg" alt="img" style="zoom:67%;" />

方法二：既然要很多episode才能收敛，那么索性我就不管它收不收敛了，还记得DP方法中，在策略提升过程中我还用到的一个方法吗？就是值迭代（value iteration），这是一个极端的例子，就是在策略估计的时候只进行了一次迭代就转向策略提升了。速度是快了，但最后精度就降低了。

<img src="https://pic4.zhimg.com/80/v2-d96b4050e9911fb73db09470461ddd5f_1440w.jpg" alt="img" style="zoom:67%;" />

再看第二个假设，怎么解决exploring starts这个假设呢？我们一方面希望能找到最好的策略，另一方面又不知道当前策略是不是最好的，这就得去尝试不同的state action pair。一旦找到了一个更好的，如果一直沿着这个策略进行下去，那就有可能陷入局部最优解，进而找不到最优解了。

为了解决这个矛盾，书中提出了两种方法，就是我们说的on-policy和off-policy。所谓的on-policy就是生成episode的策略和迭代优化的策略是同一个，并且这个策略是一种软（soft）策略，即 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+a%2Cs+%5Cright%29+%3E%5Ctext%7B0%5C+%7Dfor%5C+all%5C+s%5Cin+%5Cmathcal%7BS%7D%2Ca%5Cin+%5Cmathcal%7BA%7D+)

具体地，我们使用 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cvarepsilon+-greedy+) 策略，以下算法就是使用该策略的On-policy first-visit MC control

<img src="https://pic1.zhimg.com/80/v2-e57c20f5a2def60e305db46099455e4c_1440w.jpg" alt="img" style="zoom:67%;" />

On-policy方法在一定程度上解决了exploring starts这个假设，让策略既greedy又exploratory，最后得到的策略也一定程度上达到最优。Off-policy方法就更加直接了，分别在策略估计和策略提升的时候使用两种策略，一个具有探索性的策略专门用于产生episode积累经验，称为behavior policy ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmu+) ，另一个则是更为贪婪，用来学习成为最优策略的target policy ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+) 。为了利用从规则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+) 产生的 episodes 来评估 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+) 的value，则我们需要规则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) 下的所有行为在规则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 下被执行过，也就是要求对所有满足 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+%5Cleft%28+s%2Ca+%5Cright%29+%3E0)的 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cleft%28+s%2Ca+%5Cright%29+) 均有 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmu+%5Cleft%28+s%2Ca+%5Cright%29+%3E0+) ，这个假设可以称为是“覆盖”（coverage）。



## **Off-policy预测问题中的重要性采样（Importance Sampling）**

几乎所有off-policy方法使用的都是重要性采样，就是给定服从一种分布的样本情况下，估计另外一种分布下期望值的一般方法。我们根据轨迹在target policy和behavior policies 下发生的相关概率来对 returns 赋予权重，给定初始状态 ![[公式]](https://www.zhihu.com/equation?tex=+S_t+) ，state-action轨迹为 ![[公式]](https://www.zhihu.com/equation?tex=+A_t%2CS_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%2C..%2CS_T+) 在规则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) 下发生的概率为

![[公式]](https://www.zhihu.com/equation?tex=%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cpi+%5Cleft%28+A_k%7CS_k+%5Cright%29+p%5Cleft%28+S_%7Bk%2B1%7D%7CS_k%2CA_k+%5Cright%29%7D+)

其中，p 代表的是 state-transition 概率函数，因此，轨迹在 target policiy 和 behavior policy 下发生的相关概率（即 importance-sampling ratio）为

![[公式]](https://www.zhihu.com/equation?tex=+%5Crho+_%7Bt%3AT-1%7D%3D%5Cfrac%7B%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cpi+%5Cleft%28+A_k%7CS_k+%5Cright%29+p%5Cleft%28+S_%7Bk%2B1%7D%7CS_k%2CA_k+%5Cright%29%7D%7D%7B%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cmu+%5Cleft%28+A_k%7CS_k+%5Cright%29+p%5Cleft%28+S_%7Bk%2B1%7D%7CS_k%2CA_k+%5Cright%29%7D%7D%3D%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cfrac%7B%5Cpi+%5Cleft%28+A_k%7CS_k+%5Cright%29%7D%7B%5Cmu+%5Cleft%28+A_k%7CS_k+%5Cright%29%7D%7D+)

可以看出， ![[公式]](https://www.zhihu.com/equation?tex=+%5Crho+_%7Bt%3AT-1%7D+) 与MDP没有关系，仅仅与两个规则相关。好了，为什么我们要求这个 ![[公式]](https://www.zhihu.com/equation?tex=+%5Crho+_%7Bt%3AT-1%7D+) ？我们的目的是要估计一个规则，即求target policy下回报的期望，但现在我们是根据behavior policy生成的episode，得到的自然是一个错误的期望值 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathbb%7BE%7D%5Cleft%5B+G_t%7CS_t+%5Cright%5D+%3Dv_%7B%5Cmu%7D%5Cleft%28+S_t+%5Cright%29+) ，但我们要的是![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%5Cleft%28+S_t+%5Cright%29+) ，所以这个时候就该 ![[公式]](https://www.zhihu.com/equation?tex=+%5Crho+_%7Bt%3AT-1%7D+) 发挥作用了！

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%5Cleft%5B+%5Crho+_%7Bt%2CT-1%7DG_t%7CS_t+%5Cright%5D+%3Dv_%7B%5Cpi%7D%5Cleft%28+S_t+%5Cright%29+)

好，接下来就要给出off-policy方法是如何评估策略的公式。现在假设我们有了一系列服从规则 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 的episodes，首先我们对这些 episodes 进行连接和标号，假设第一个 episode 在时刻 100 结束，则第二个 episode 就以时间 101 开始，以此类推。下面规定一些符号表示：

- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29+) :对 every-visit 方法，它代表所有状态 s 被 visit 的时刻的集合，对 first-visit 方法，它仅代表所有状态 s 在某个 episode 中第一次被 visit 的时刻的集合；
- ![[公式]](https://www.zhihu.com/equation?tex=T%5Cleft%28+t+%5Cright%29+) :从时刻t到 ![[公式]](https://www.zhihu.com/equation?tex=+T%5Cleft%28+t+%5Cright%29+) 的回报（return）；
- ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+G_t+%5Cright%5C%7D+_%7Bt%5Cin+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29%7D+) :属于状态s的回报；
- ![[公式]](https://www.zhihu.com/equation?tex=+%5Cleft%5C%7B+%5Crho+_%7Bt%3AT-1%7D+%5Cright%5C%7D+_%7Bt%5Cin+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29%7D+) :代表相应的importance-sampling ratio;

根据求平均的方法不同，有两种估计 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+) ，一种是ordinary importance sampling：

![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28+s+%5Cright%29+%3D%5Cfrac%7B%5Csum_%7Bt%5Cin+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29%7D%7B%5Crho+_%7Bt%3AT%5Cleft%28+t+%5Cright%29+-1%7DG_t%7D%7D%7B%5Cleft%7C+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29+%5Cright%7C%7D+)

另一种是weighted importance sampling,

![[公式]](https://www.zhihu.com/equation?tex=+V%5Cleft%28+s+%5Cright%29+%3D%5Cfrac%7B%5Csum_%7Bt%5Cin+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29%7D%7B%5Crho+_%7Bt%3AT%5Cleft%28+t+%5Cright%29+-1%7DG_t%7D%7D%7B%5Csum_%7Bt%5Cin+%5Cmathcal%7BT%7D%5Cleft%28+s+%5Cright%29%7D%7B%5Crho+_%7Bt%3AT%5Cleft%28+t+%5Cright%29+-1%7D%7D%7D+)



## **增量式求均值**

在策略估计的时候，MC所采用的方法是将所有样本episodes所得到的回报求均值，怎么求这个均值呢？当然可以简单的求和再除以episode的数量。这里要介绍的是另一种方法——增量式求均值。

假设我们得到了一系列回报 ![[公式]](https://www.zhihu.com/equation?tex=+G_1%2CG_2%2C...%2CG_%7Bt-1%7D+) ，对于off-policy来说，因为我们利用了重要性采样，所以多了一个权重的因素，设每个回报的权重为 ![[公式]](https://www.zhihu.com/equation?tex=W_k+)

![[公式]](https://www.zhihu.com/equation?tex=V_n%3D%5Cfrac%7B%5Csum_%7Bk%3D1%7D%5E%7Bn-1%7D%7BW_kG_k%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7Bn-1%7D%7BW_k%7D%7D+)

于是有

![[公式]](https://www.zhihu.com/equation?tex=+V_%7Bn%2B1%7D%3DV_n%2B%5Cfrac%7BW_n%7D%7BC_n%7D%5Cleft%28+G_n-V_n+%5Cright%29+)

![[公式]](https://www.zhihu.com/equation?tex=+C_%7Bn%2B1%7D%3DC_n%2BW_%7Bn%2B1%7D+)



## **Off-policy的控制问题**

前面两个小节就是在为这个小节做铺垫，所以直接给出算法啦。

<img src="https://pic3.zhimg.com/80/v2-70ff3b8780785f972d118ec74a926232_1440w.jpg" alt="img" style="zoom:67%;" />

## 



# 强化学习——时序差分算法

1. 理解TD(0)的预测（prediction）问题;
2. On-policy控制（control）算法SARSA；
3. Off-policy控制（control）算法Q-learning；
4. TD算法相对于MC算法和DP算法的优势；



时序差分（Temporal-Difference）算法应该是强化学习中最为核心的算法了，它结合了前面讲到的动态规划和蒙特卡洛算法。如蒙特卡洛算法一样，它不需要知道具体的环境模型，可以直接从经验中学习；另一方面，继承了DP算法的自举（bootstrap）方法，可以利用学到的估计值来更新，而不用等到一个episode结束后再更新。三种算法的control问题，即找到最优策略都是使用广义的策略迭代，主要不同在于prediction问题，即根据当前策略估计值函数。



## **TD算法的Prediction问题**

如果还不清楚prediction和control问题具体指什么，可以看看我[上一篇文章](https://zhuanlan.zhihu.com/p/34395444)。TD算法和MC算法都是基于经验去解决预测问题，给出一些基于策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+) 的样本估计状态 ![[公式]](https://www.zhihu.com/equation?tex=+S_t+) 下的值函数 ![[公式]](https://www.zhihu.com/equation?tex=+v_%7B%5Cpi%7D+) 。**MC算法是等到一个episode结束得到return以后再借此更新** ![[公式]](https://www.zhihu.com/equation?tex=+V%5Cleft%28+S_t+%5Cright%29+) ，如下图所示



<img src="https://pic1.zhimg.com/80/v2-3982df808a8377b0b8d238d62242ec38_1440w.jpg" alt="img" style="zoom:67%;" />



**TD算法在这里就不同，它是每过一个time step就更新一次，利用奖励** ![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+) **和值函数** ![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28+S_%7Bt%2B1%7D+%5Cright%29+) ，当然，这里所说的one-step TD 方法，也可以两步一更新，三步一更新…..是不是等到N步以后再更新就是蒙特卡洛算法了？对，就是这么一回事。



<img src="https://pic4.zhimg.com/80/v2-21c6a5351961bae06fd787b89b0d8b93_1440w.jpg" alt="img" style="zoom:67%;" />



既然这两种方法的backup图都贴出来了，也把DP算法的图贴出来对比着看吧。



<img src="https://pic4.zhimg.com/80/v2-e2a7ad2d7a6cc039ff40743e438d427f_1440w.jpg" alt="img" style="zoom:67%;" />



TD(0)算法的伪代码如下



<img src="https://pic2.zhimg.com/80/v2-af187b15f3c4fc772d4d06083000b849_1440w.jpg" alt="img" style="zoom: 80%;" />



其中 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cdelta+%3DR%2B%5Cgamma+V%5Cleft%28+S%27+%5Cright%29+-V%5Cleft%28+S+%5Cright%29+) 称为TD误差（error）。

比较了三种算法的不同，那TD算法的优势何在？要回答这个问题，可能得写本书了，我这里只简单提几点，更多的还得去看书和看文献。很显然，TD算法相对于DP算法优势在于它**不需要知道环境的模型**，这也是符合实际问题的，绝大部分情况下我们都不可能建立一个精确的模型来描述一个问题。相较于MC算法，**TD算法不需要等到episode最后才能更新值函数，而是一个time step就可以更新一次。**因为有可能一个episode会花很长时间，比如玩一个游戏，需要几个小时才能结束一个回合，那么我几个小时才能得到一个回报，训练完一个算法不知道要花多久了。换句话说，MC算法训练成本太高，这点上TD算法就显得很有优势了。

那在有限的数据下，TD算法一定能收敛吗？答案是一定会收敛，经过证明只要step size足够小，对于任何策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+) ，TD(0)都能收敛到 ![[公式]](https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D+) 。那么这三种方法谁收敛的更快一些呢？这就不一定了，好像没有谁在数学上证明过在收敛速度上谁更有优势。



## **TD算法的Control问题**

## **Sarsa算法**

在面对exploration和exploitation的两难时，算法又分为了两类：on-policy和off-policy算法。Sarsa是一种on-policy TD算法。我们去学习一个最优的动作-状态值函数而不是状态值函数。

![[公式]](https://www.zhihu.com/equation?tex=+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cgets+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%2B%5Calpha+%5Cleft%5B+R_%7Bt%2B1%7D%2B%5Cgamma+Q%5Cleft%28+S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D+%5Cright%29+-Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cright%5D+)

Sarsa算法的原始策略和更新策略是一致的，而其更新策略和MC不一样的是其策略更新不需要采样一个完整的轨迹，在执行完一个动作后就可以更新其值函数。如果 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D+) 是终止状态，则 ![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D+%5Cright%29+) 定义为0。之所以叫Sarsa，是因为迭代算法中的五个元素 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+S_t%2CA_t%2CR_%7Bt%2B%5Ctext%7B1%2C%7D%7DS_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D+%5Cright%29+) 。其伪代码如下图：



<img src="https://pic4.zhimg.com/80/v2-1d58c60daf7a85e4064c76384f0ac397_1440w.jpg" alt="img" style="zoom:80%;" />



## **Q-learning算法**

Q-learning是一种off-policy算法，更新方法如下

![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cgets+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%2B%5Calpha+%5Cleft%5B+R_%7Bt%2B1%7D%2B%5Cgamma+%5Cunderset%7Ba%7D%7B%5Cmax%7DQ%5Cleft%28+S_%7Bt%2B1%7D%2Ca+%5Cright%29+-Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cright%5D+)



<img src="https://pic1.zhimg.com/80/v2-3919d5ce017f134a073c8fcf956503a8_1440w.jpg" alt="img" style="zoom:80%;" />



与Sarsa只有一点不同，就是在更新 ![[公式]](https://www.zhihu.com/equation?tex=+S_t+) 的动作值函数的时候需要 ![[公式]](https://www.zhihu.com/equation?tex=+S_%7Bt%2B1%7D+) 的动作值函数，那状态 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D+) 下的动作怎么选呢？Sarsa用的是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+-greedy+) 方法，和选择 ![[公式]](https://www.zhihu.com/equation?tex=S_t+) 下的动作一样；而Q-learning是用的 ![[公式]](https://www.zhihu.com/equation?tex=greedy+) 方法，和选择 ![[公式]](https://www.zhihu.com/equation?tex=S_t+) 下的动作不一样，因此称为off-policy。



## **总结**

本文简单地介绍了时序差分算法，在prediction问题方面着重与MC和DP方法作比较，因为它就是两者的结合。在control问题上分别介绍了on-policy方法：Sarsa和off-policy方法：Q-learning。当然，算法肯定不只有这两种，比如还有Expected Sarsa和Double Q-learning等等。除了on-policy和off-policy方法外，还有一种actor-critic方法，这是后面要学习的内容，具体到更为细分的算法，那就太多太多了……



# 强化学习——从Q-Learning到DQN到底发生了什么？

[强化学习——从Q-Learning到DQN到底发生了什么？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/35882937)

\1. 复习Q-Learning；

\2. 理解什么是值函数近似（Function Approximation）；

\3. 理解什么是DQN，弄清它和Q-Learning的区别是什么。



## **用Q-Learning解决经典迷宫问题**

现有一个5房间的房子，如图1所示，房间与房间之间通过门连接，编号0到4,5号是房子外边，即我们的终点。我们将agent随机放在任一房间内，每打开一个房门返回一个reward。图2为房间之间的抽象关系图，箭头表示agent可以从该房间转移到与之相连的房间，箭头上的数字代表reward值。

![img](https://pic2.zhimg.com/80/v2-9be995fd2d20723bc11d487df8c4c549_1440w.jpg)

![img](https://pic1.zhimg.com/80/v2-1f67985d9a97f975c8a5152bcc8fcec0_1440w.jpg)

根据此关系，可以得到reward矩阵为

![img](https://pic3.zhimg.com/80/v2-0cdb9f39735da4c2c1c6f86101fc03d2_1440w.jpg)

Q-Learning是一种off-policy TD方法，伪代码如图所示

![img](https://pic1.zhimg.com/80/v2-3919d5ce017f134a073c8fcf956503a8_1440w.jpg)Q-Learning伪代码

我们首先会**初始化一个Q表**，用于**记录状态-动作对的值**，每个episode中的每一步都会根据下列公式更新一次Q表

![[公式]](https://www.zhihu.com/equation?tex=+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cgets+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%2B%5Calpha+%5Cleft%5B+R_%7Bt%2B1%7D%2B%5Cgamma+%5Cunderset%7Ba%7D%7B%5Cmax%7DQ%5Cleft%28+S_%7Bt%2B1%7D%2Ca+%5Cright%29+-Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cright%5D+)

这里的迷宫问题，每一次episode的结束指的是到达终点状态5。为了简单起见，这里将学习率 ![[公式]](https://www.zhihu.com/equation?tex=+%5Calpha+) 设为1，更新公式变为

![[公式]](https://www.zhihu.com/equation?tex=+Q%5Cleft%28+S_t%2CA_t+%5Cright%29+%5Cgets+R_%7Bt%2B1%7D%2B%5Cgamma+%5Cunderset%7Ba%7D%7B%5Cmax%7DQ%5Cleft%28+S_%7Bt%2B1%7D%2Ca+%5Cright%29+)

另外，将衰减系数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cgamma+) 设为0.8。Q表初始化为一个5×5的全0矩阵。下面这个小视频展示了一个episode中一步的更新过程，每次这样更新，最终Q表会收敛到一个矩阵。

视频演示：[知乎视频 (zhihu.com)](https://video.zhihu.com/video/970368234306883584?player={"autoplay"%3Afalse%2C"shouldShowPageFullScreenButton"%3Atrue}")

<iframe title="video" src="https://video.zhihu.com/video/970368234306883584?player=%7B%22autoplay%22%3Afalse%2C%22shouldShowPageFullScreenButton%22%3Atrue%7D" allowfullscreen="" frameborder="0" class="css-uwwqev" style="width: 688px; height: 387px;"></iframe>

<img src="C:\Users\jc\AppData\Roaming\Typora\typora-user-images\image-20220301100438495.png" alt="image-20220301100438495" style="zoom: 50%;" />



最终Q表收敛为

![img](https://pic2.zhimg.com/80/v2-23cc67130a8032ee810374e1edd4e9b9_1440w.jpg)

因此，也可以得到最优路径如下红色箭头所示

![img](https://pic3.zhimg.com/80/v2-7a3299704e5c588e9d0995b1465e0e56_1440w.jpg)

Q-Learning方法很好的解决了这个迷宫问题，但是这终究只是一个小问题（状态空间和动作空间都很小），**实际情况下，大部分问题都是有巨大的状态空间或者动作空间，想建立一个Q表，内存是绝对不允许的，而且数据量和时间开销也是个问题。**

##  **值函数近似与DQN**

值函数近似（Function Approximation）的方法就是为了解决状态空间过大，也称为“维度灾难”的问题。通过用**函数**而不是Q表来表示 ![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+s%2Ca+%5Cright%29+) ，这个函数可以是线性的也可以使非线性的。

![[公式]](https://www.zhihu.com/equation?tex=+%5Chat%7Bv%7D%5Cleft%28+s%2C%5Cboldsymbol%7Bw%7D+%5Cright%29+%5Capprox+v_%7B%5Cpi%7D%5Cleft%28+s+%5Cright%29+%5C+or%5C+%5Chat%7Bq%7D%5Cleft%28+s%2Ca%2C%5Cboldsymbol%7Bw%7D+%5Cright%29+%5Capprox+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+)

其中 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D+) 称为“权重”。那怎么把这个权重求出来，即拟合出这样一个合适的函数呢？**这里就要结合机器学习算法里的一些有监督学习算法，对输入的状态提取特征作为输入，通过MC/TD计算出值函数作为输出，然后对函数参数** ![[公式]](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D+) **进行训练，直到收敛。**这里主要说的是回归算法，比如线性回归、决策树、**神经网络**等。

这里，就可以引入DQN（Deep Q-Network）了，**实际上它就是Q-Learning和神经网络的结合，将Q-Learning的Q表变成了Q-Network。**

好，现在关键问题来了。这么去训练这个网络呢？换句话说，怎么去确定网络参数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D+) 呢？**第一，我们需要一个Loss Function；第二，我们需要足够的训练样本。**

训练样本好说，通过 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+-greedy+) 策略去生成就好。回忆一下Q-Learning，我们更新Q表是利用每步的reward和当前Q表来迭代的。那么我们可以用这个计算出来的Q值作为监督学习的“标签”来设计Loss Function，我们采用如下形式，即近似值和真实值的均方差

![[公式]](https://www.zhihu.com/equation?tex=+J%5Cleft%28+%5Cboldsymbol%7Bw%7D+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cboldsymbol%7B%5Cpi+%7D%7D%5Cleft%5B+%5Cleft%28+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+-%5Chat%7Bq%7D%5Cleft%28+s%2Ca%2C%5Cboldsymbol%7Bw%7D+%5Cright%29+%5Cright%29+%5E2+%5Cright%5D+)

采用随机梯度下降法（SGD）来迭代求解，得到我们想要的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboldsymbol%7Bw%7D+) ，具体公式和过程还请看参考资料，这里不展开了，其实就是求导啦。值得一提的是，上述公式中的 ![[公式]](https://www.zhihu.com/equation?tex=+q_%7B%5Cpi%7D%5Cleft%28+s%2Ca+%5Cright%29+) 根据不同方法算出来，其形式不一样，比如利用MC，则为 ![[公式]](https://www.zhihu.com/equation?tex=+G_t+) （回报）；利用TD(0)，则为 ![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2B%5Cgamma+%5Chat%7Bq%7D%5Cleft%28+s_%7Bt%2B1%7D%2Ca_%7Bt%2B1%7D%2C%5Cboldsymbol%7Bw%7D+%5Cright%29+) ；Q-Learning呢，就是![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2B%5Cgamma+%5Cmax+%5Chat%7Bq%7D%5Cleft%28+s_%7Bt%2B1%7D%2Ca_%7Bt%2B1%7D%2C%5Cboldsymbol%7Bw%7D+%5Cright%29+) 。



在David Silver的课里，他根据每次更新所参与样本量的不同把更新方法分为增量法（Incremental Methods）和批处理法（Batch Methods）。前者是来一个数据就更新一次，后者是先攒一堆样本，再从中采样一部分拿来更新Q网络，称之为“**经验回放**”，实际上DeepMind提出的DQN就是采用了经验回放的方法。为什么要采用经验回放的方法？因为对神经网络进行训练时，**假设样本是独立同分布的**。而通过强化学习采集到的数据之间存在着关联性，利用这些数据进行顺序训练，神经网络当然不稳定。经验回放可以打破数据间的关联。

最后附上DQN的伪代码

![img](https://pic1.zhimg.com/80/v2-07095987f15891afb6290a0a27877fc4_1440w.jpg)DQN伪代码

学到这里，其实可以做一个阶段性总结了，强化学习算法的基本框架可以用下图概括

![img](https://pic4.zhimg.com/80/v2-da914a9774f409116731f3a2492be9f3_1440w.jpg)

​																					强化学习框架

# 强化学习——策略梯度与Actor-Critic算法

（1） 理解基于值函数的方法（Value-Based method）和基于策略的方法（Policy-Based method）的不同；

（2） 理解策略梯度法（Policy Gradient）的目标函数和优化思路；

（3） 理解蒙特卡洛策略梯度和Actor-Critic方法；



##  **基于策略的方法**



前面学习的一些强化学习算法比如Q-Learning，SARSA，DQN都是基于价值的算法，即得到Q表或者近似一个价值函数，然后根据这个学到的Q表或者Q函数来制定策略。而本文要介绍的算法就是另一个思路了，**直接学习策略函数**。

![img](https://pic2.zhimg.com/80/v2-74ab32eb03cd93bb907f413366576ebd_1440w.jpg)

基于价值的方法比如DQN，输入的是状态，输出的是Q值，有了状态-动作对到Q值的映射，在某个状态下做决策的时候就选择Q值最大的动作即可，这就是DQN的策略。但是有这样几个问题：

① 在估计值函数的时候一个任意小的变化可能导致对应动作被选择或者不被选择，这种不连续的变化是致使基于值函数的方法无法得到收敛保证的重要因素。

② 选择最大的Q值这样一个搜索过程在高纬度或者连续空间是非常困难的；

③ 无法学习到随机策略，有些情况下随机策略往往是最优策略。以David Silver课件中的一个例子说明：

![img](https://pic1.zhimg.com/80/v2-2d45f4ee327db8ec1be6a1b2d556dd2c_1440w.jpg)

agent在迷宫中移动寻找宝藏，由于迷宫的对称结构，当agent处在灰色格子上时是无法分辨自己处于哪个灰色格子上的，所以最后学习到的策略可能是这样：

![img](https://pic1.zhimg.com/80/v2-1144216c6d0fa896ebaed75cb2099170_1440w.jpg)

当初始位置在第一个白格子上，就会陷入死循环，但是随机策略就会好一些：

![img](https://pic1.zhimg.com/80/v2-10f372bd6502fe9bc0a4c8ae07c59050_1440w.jpg)

因为学出来的策略不是确定性输出一个动作，而是动作的概率。

针对以上问题，有人就提出了基于策略的方法，即输入状态，直接学习策略。那我们下面就来揭开策略梯度的面纱。



## **3 策略梯度**



思路很直接，我们的最终目标是使回报最大，于是就冲着这个目标制定**目标函数** ![[公式]](https://www.zhihu.com/equation?tex=+J%5Cleft%28+%5Ctheta+%5Cright%29+) **，**通过训练，得到一个参数化的策略函数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cmathbb%7BP%7D%5Cleft%5B+a%7Cs%2C%5Ctheta+%5Cright%5D+) 。对于不同的问题类型，有不同的目标函数可以选择：

① 在能够产生完整Episode的环境下，可以使用start value

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+J_1%5Cleft%28+%5Ctheta+%5Cright%29+%3DV%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s_1+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+v_1+%5Cright%5D+%5C%5C)

② 在连续的环境下，可以使用average value

![[公式]](https://www.zhihu.com/equation?tex=+%5C%5CJ_%7BavV%7D%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Csum_s%7Bd%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29%7D+%5C%5C)

③ 或者使用Average reward per time-step

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+J_%7BavR%7D%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Csum_s%7Bd%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29%7D%5Csum_a%7B%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+%5Cmathcal%7BR%7D_%7Bs%7D%5E%7Ba%7D%7D+%5C%5C)

有了目标函数，我们当前的任务就是使目标函数最大化，也就是寻得一组参数向量 ![[公式]](https://www.zhihu.com/equation?tex=+%5Ctheta+) ，使得目标函数最大。**这实际上做的是改变策略概率而非改变行动轨迹**。很自然的，下一步就是使用梯度下降（上升）法来完成这个工作，问题就转向 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+) 的求解。



我就不废话了，直接给出**策略梯度定理**：

对于任何可微的策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+) ，对于任何策略的目标函数 ![[公式]](https://www.zhihu.com/equation?tex=J_1%2CJ_%7BavR%7D+) 或者 ![[公式]](https://www.zhihu.com/equation?tex=J_%7BavV%7D%2F%5Cleft%28+1-%5Cgamma+%5Cright%29+) ，其梯度都如下式所示，转换成了**策略梯度**：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+Q%5E%7B%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+%5Ctheta+%5Cright%29%7D%5Cleft%28+s%2Ca+%5Cright%29+%5Cright%5D+%5C%5C)

回想一下监督学习算法，我们有大量的样本数据，并且数据都有标签，于是在训练的时候知道当前参数下得到的结果是好是坏（能与监督信息对比），然后可以根据对比结果调整参数优化的方向（梯度）。而强化学习里没有一个监督信息告诉我们当前的策略是好是坏，**我们得去计算价值函数来起到标签的作用**。

一般地，针对离散行为常用**softmax策略**，即最后得到的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+) 是每一个离散行为应该以怎样的概率来执行。而对于连续问题，则使用**高斯策略**，因为行为对应于某一个数值，所以学习到的策略是一个高斯分布，通常是对该分布的均值进行参数化表示。



## **4 蒙特卡洛策略梯度（REINFORCE）**



算法描述如下：

![img](https://pic1.zhimg.com/80/v2-10cc9207d266504673fbf0951492c030_1440w.jpg)

首先随机初始化参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+) ，对每个episode，计算其t=1到t=T-1的return ![[公式]](https://www.zhihu.com/equation?tex=+v_t+) ，然后使用随机梯度上升法更新参数。对于策略梯度定理公式里的期望，我们通过采样的形式来替代，即使用t时刻的return作为当前策略下动作价值函数的**无偏估计**。

但是，REINFORCE存在如下三个问题：



① 由于agent在一个episode中会采取很多动作，我们很难说哪个动作对最后结果是有用的，换句话说，这种算法存在高方差（variance）;

② 收敛速度慢；

③ 只在这种episodic环境下能用。

为了解决上述问题，于是提出了Actor-Critic算法。



## **5 Actor-Critic算法**



Actor是演员的意思，Critic是评论家的意思，顾名思义，这种算法就是通过引入一种评价机制来解决高方差的问题。具体来说，Critic就类似于**策略评估**，去估计动作值函数：

![[公式]](https://www.zhihu.com/equation?tex=+%5C%5CQ_w%5Cleft%28+s%2Ca+%5Cright%29+%5Capprox+Q%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+%5C%5C)

于是，Actor-Critic算法中就有两组参数：

Critic：更新动作值函数参数 ![[公式]](https://www.zhihu.com/equation?tex=w+) ；

Actor：以Critic所指导的方向更新策略参数 ![[公式]](https://www.zhihu.com/equation?tex=+%5Ctheta+) 。

所以说，Actor-Critic算法是一种近似的策略梯度

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+%5Capprox+%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+Q_w%5Cleft%28+s%2Ca+%5Cright%29+%5Cright%5D+%5C%5C)

策略评估在前几讲都有提到，也就是衡量一个策略好坏的过程，可以是MC策略评估，TD或者TD（ ![[公式]](https://www.zhihu.com/equation?tex=+%5Clambda+) ）。举个例子，假设用一个线性函数来近似动作值函数 ![[公式]](https://www.zhihu.com/equation?tex=+Q_w%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cphi+%5Cleft%28+s%2Ca+%5Cright%29+%5ETw+) ，那么Critic过程就是用线性TD（0）来更新 ![[公式]](https://www.zhihu.com/equation?tex=+w+) ，Actor过程就是用策略梯度来更新

![[公式]](https://www.zhihu.com/equation?tex=+%5Ctheta+)，具体伪代码如下：

![img](https://pic4.zhimg.com/80/v2-9bf0372db12a3d157187d06dc9653d0b_1440w.jpg)

前面提到了，**这种算法实际上用了一个近似的策略梯度，这样会引入偏差（bias）**，导致最后无法收敛到一个合适的策略，一个解决方法就是设计 ![[公式]](https://www.zhihu.com/equation?tex=Q_w%5Cleft%28+s%2Ca+%5Cright%29+) 时满足下面两个条件（Compatible Function Approximation Theorem）：

① 近似价值函数的梯度完全等同于策略函数对数的梯度

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C%5Cnabla+_wQ_w%5Cleft%28+s%2Ca+%5Cright%29+%3D%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+%5C%5C)

② 值函数参数 ![[公式]](https://www.zhihu.com/equation?tex=w+) 使得均方差最小

![[公式]](https://www.zhihu.com/equation?tex=+%5C%5C%5Cvarepsilon+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cleft%28+Q%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+-Q_w%5Cleft%28+s%2Ca+%5Cright%29+%5Cright%29+%5E2+%5Cright%5D+%5C%5C)

满足以上两个条件，那么 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+Q_w%5Cleft%28+s%2Ca+%5Cright%29+%5Cright%5D+) 。简单的证明如下：

![img](https://pic4.zhimg.com/80/v2-b9061b4382189cd50b86627079dd2fdb_1440w.jpg)

## **6 Actor-Critic with Baseline**



除了引入Critic过程来减小方差，同时也可以采用一种从 ![[公式]](https://www.zhihu.com/equation?tex=+Q%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+) 减去Baseline的方法来减小方差。具体地，就是从策略梯度中减去baseline函数 ![[公式]](https://www.zhihu.com/equation?tex=+B%5Cleft%28+s+%5Cright%29+) ，要求这一函数仅与状态有关，与行为无关，因而这样就不会改变梯度本身。

![img](https://pic2.zhimg.com/80/v2-1d2ddf0f0c1582dbd8e1de1e1f68b7e9_1440w.jpg)

![[公式]](https://www.zhihu.com/equation?tex=+B%5Cleft%28+s+%5Cright%29+) 一个比较好的选择是状态值函数 ![[公式]](https://www.zhihu.com/equation?tex=+V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+) ，基于以上讨论，我们引入一个**advantage function函数** ![[公式]](https://www.zhihu.com/equation?tex=+A%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+) ，定义

![[公式]](https://www.zhihu.com/equation?tex=%5C%5CA%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+%3DQ%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+-V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=+%5C%5C%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+A%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+%5Cright%5D+%5C%5C)

好了，接下来的问题就是 ![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+) 怎么去近似了，因为它既有动作值函数，又有状态值函数，是不是就需要两组参数来分别近似这两个函数呢？可以这样做，但一般我们用TD error来近似 ![[公式]](https://www.zhihu.com/equation?tex=A%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+) ，因为它是 ![[公式]](https://www.zhihu.com/equation?tex=+A%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+) 的无偏估计。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+%5Cdelta+%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%3Dr%2B%5Cgamma+V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%27+%5Cright%29+-V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cdelta+%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%7Cs%2Ca+%5Cright%5D+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+r%2B%5Cgamma+V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%27+%5Cright%29+%7Cs%2Ca+%5Cright%5D+-V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+%5C%5C+%3DQ%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+-V%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s+%5Cright%29+%5C%5C+%3DA%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%28+s%2Ca+%5Cright%29+)

因此有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+_%7B%5Ctheta%7DJ%5Cleft%28+%5Ctheta+%5Cright%29+%3D%5Cmathbb%7BE%7D_%7B%5Cpi+_%7B%5Ctheta%7D%7D%5Cleft%5B+%5Cnabla+_%7B%5Ctheta%7D%5Clog+%5Cpi+_%7B%5Ctheta%7D%5Cleft%28+s%2Ca+%5Cright%29+%5Cdelta+%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D+%5Cright%5D+) ，在实际应用中，可以采用近似的TD error，即 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+%5E%7B%5Cpi+_%7B%5Ctheta%7D%7D%3Dr%2B%5Cgamma+V%5Cleft%28+s%27+%5Cright%29+-V%5Cleft%28+s+%5Cright%29+) 。

不论是Actor还是Critic，我们都可以在不同时间尺度（time-scales）上近似策略梯度或是值函数。直接引用David课程的几页PPT：

![img](https://pic4.zhimg.com/80/v2-e373d99713e3340111b958fc086849cf_1440w.jpg) 

![img](https://pic1.zhimg.com/80/v2-7e08ebcce363286be58ab533fe65d8d0_1440w.jpg)

最后课程还提了一下**Natural Actor Critic算法**，这里就不展开说了。



## **7 总结**



这一节应该是目前最难理解的一个小节了，需要重点掌握的是Actor-Critic算法，其核心是策略梯度定理。简单的说，以前基于值函数近似的算法是先近似出一个Q函数，然后用 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cvarepsilon+-greedy+) 策略去选择要执行的动作，由于种种不足，现在我们直接近似出一个策略函数，蒙特卡洛策略梯度算法虽然计算的策略梯度是无偏的，但是方差大，速度慢，因此我们通过减去Baseline来减小方差，并且引入Critic过程进一步优化，形成了Actor-Critic算法，其中也包括了策略梯度的近似，这样我们就可以利用其能够Bootstrapping的性质加快算法速度，并能够在non-episodic环境下得以使用。策略梯度定理帮助我们把Q函数和策略梯度联系了起来，但是我们并不知道真实的Q函数，所以还得去近似估计，想要得到一个无偏的估计，就得满足Compatible Function Approximation Theorem。



Critic实际上就是策略评估，它引导Actor参数梯度方向走向更好。最后引用David的一页PPT作为结束吧。

![img](https://pic2.zhimg.com/80/v2-601f557732c6f17c43629d501c0cd1dd_1440w.jpg)





## 







[lizzieyzy/readme_cn.pdf at main · yzyray/lizzieyzy · GitHub](https://github.com/yzyray/lizzieyzy/blob/main/readme_cn.pdf)





# 2022-03-02

# 深度学习训练之Batch

[深度学习训练之Batch - 简书 (jianshu.com)](https://www.jianshu.com/p/71f31c105879)

![img](https://upload-images.jianshu.io/upload_images/4563513-54bf713d97ff1d53.png?imageMogr2/auto-orient/strip|imageView2/2/w/557/format/webp)

1、对于一个有 2000 个训练样本的数据集。将 2000 个样本分成大小为 500 的 batch，那么完成一个 epoch 需要 4 个 iteration。

2、如果把准备训练数据比喻成一块准备打火锅的牛肉，那么epoch就是整块牛肉，batch就是切片后的牛肉片，iteration就是涮一块牛肉片

<img src="https://upload-images.jianshu.io/upload_images/4563513-78f08e224cb7088e.png?imageMogr2/auto-orient/strip|imageView2/2/w/845/format/webp" alt="img" style="zoom:50%;" />

### 二、Batch用来干什么

不是给人吃，是喂给模型吃。在搭建了“[模型](https://www.jianshu.com/p/1b05f1d9b125)-[策略](https://www.jianshu.com/p/1620343c9f3c)-[算法](https://www.jianshu.com/p/3f3ce11eeb85)”三大步之后，要开始利用数据跑（训练）这个框架，训练出最佳参数。

1. 理想状态，就是把所有数据都喂给框架，求出最小化损失，再更新参数，重复这个过程，但是就像煮一整块牛肉那样，不知道什么时候才有得吃。----全量数据的梯度下降算法
2. 另一个极端的状态，就是每次只给模型喂一条数据，立马就熟了，快是够快了，但是一个不小心也会直接化掉，吃都没得吃（可能无法得到局部最优）----随机梯度下降算法（stochastic gradient descent）
3. 平衡方案，综合考虑又要快，又要有得吃，那么选用切片涮牛肉的方法，把数据切成batch大小的一块，每次（iteration）只吃一块。每次只计算一小部分数据的损失函数，并更改参数。



