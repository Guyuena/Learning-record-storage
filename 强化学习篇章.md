

# 强化学习(十七) 基于模型的强化学习与Dyna算法框架

在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。

　　　　本篇主要参考了UCL强化学习课程的第8讲和Dyna-2的[论文](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications_files/dyna2.pdf)。

##  1. 基于模型的强化学习简介

　　　　基于价值的强化学习模型和基于策略的强化学习模型都不是基于模型的，它们从价值函数，策略函数中直接去学习，不用学习环境的状态转化概率模型，即在状态s下采取动作a,转到下一个状态s'的概率
$$
P_{ss'}^a
$$


　　　　而基于模型的强化学习则会尝试**从环境的模型去学习**，一般是下面两个相互独立的模型：一个是状态转化预测模型，输入当前状态s和动作a，预测下一个状态s'。另一个是奖励预测模型，输入当前状态s和动作a，预测环境的奖励r。即模型可以描述为下面两个式子：
$$
S_{t+1} \sim P(S_{t+1}|S_t,A_t)
$$

$$

R_{t+1} \sim R(R_{t+1}|S_t,A_t)
$$

如果模型P,R可以准确的描述真正的环境的转化模型，那么我们就可以基于模型来预测，**当有一个新的状态S和动作a到来时，我们可以直接基于模型预测得到新的状态和动作奖励，不需要和环境交互**。当然如果我们的模型不好，那么基于模型预测的新状态和动作奖励可能错的离谱。

　　　　从上面的描述我们可以看出基于模型的强化学习和不基于模型的强化学习的**主要区别**：即基于模型的强化学习是从模型中学习，而不基于模型的强化学习是从和环境交互的经历去学习。

　　　　下面这张图描述了基于模型的强化学习的思路：

![img](https://ask.qcloudimg.com/http-save/yehe-2770378/92c8ps58to.png?imageView2/2/w/1620)

需要通过和环境交互来对环境进行建模，然后再利用这个模型做出动作规划或者策略选择

## 2. 基于模型的强化学习算法训练流程

　　　　这里我们看看基于模型的强化学习算法训练流程，其流程和我们监督学习算法是非常类似的。

　　　　假设训练数据是若干组这样的经历：
$$
S_1,A_1,R_2,S_2,A_2,R_2,...,S_T
$$
　　　　对于每组经历，我们可以将其转化为T-1组训练样本，即：
$$
S_1,A_1 \to S_2,\;S_1,A_1 \to R_2
$$

$$
S_2,A_2 \to S_3,\;S_2,A_2 \to R_3
$$

$$
S_{T-1},A_{T-1} \to S_T,\;S_{T_1},A_{T-1} \to R_T
$$

　　　　左边的训练样本一起组成了一个分类模型或密度估计模型，输入状态和动作，输出下一个状态。 右边的训练样本一起组成了一个回归模型训练集，输入状态和动作，输出动作奖励值。

　　　　至此我们的强化学习求解过程和传统的监督学习算法没有太多区别了，可以使用传统的监督学习算法来求解这两个模型。

　　　　当然还可以更简单，即通过对训练样本进行查表法进行统计，直接得到
$$
P(S_{t+1}|S_t,A_t)
$$
的概率和
$$
R(R_{t+1}|S_t,A_t)
$$
的平均值，这样就可以直接预测。比使用模型更简单。

　　　　此外，还有其他的方法可以用来得到
$$
P(S_{t+1}|S_t,A_t)
$$
和
$$
R(R_{t+1}|S_t,A_t)
$$
，这个我们后面再讲。

　　　　虽然基于模型的强化学习思路很清晰，而且还有不要和环境持续交互优化的优点，但是用于实际产品还是有很多差距的。主要是我们的模型绝大多数时候不能准确的描述真正的环境的转化模型，那么使用基于模型的强化学习算法得到的解大多数时候也不是很实用。那么是不是基于模型的强化学习就不能用了呢？也不是，我们可以将基于模型的强化学习和不基于模型的强化学习集合起来，取长补短，这样做最常见的就是Dyna算法框架。

## 3. Dyna算法框架

　　　　Dyna算法框架并不是一个具体的强化学习算法，而是一类算法框架的总称。Dyna将基于模型的强化学习和不基于模型的强化学习集合起来，既从模型中学习，也从和环境交互的经历去学习，从而更新价值函数和（或）策略函数。如果用和第一节类似的图，可以表示如下图，和第一节的图相比，多了一个“Direct RL“的箭头，这正是不基于模型的强化学习的思路。

![img](https://ask.qcloudimg.com/http-save/yehe-2770378/ejsjr1j1ao.png?imageView2/2/w/1620)

 　　　Dyna算法框架和不同的具体的不基于模型的强化学习一起，可以得到具体的不同算法。如果我们使用基于价值函数的Q-Learning，那么我们就得到了Dyna-Q算法。我们基于Dyna-Q来看看Dyna算法框架的一般流程.

## 4. Dyna-Q算法流程

　　　　这里我们给出基于价值函数的Dyna-Q算法的概要流程。假设模型使用的是查表法。

　　　　1. 初始化任意一个状态$s$,和任意一个动作$a$对应的状态价值$Q(s,a)$, 初始化奖励模型$R(s,a)$和状态模型$P(s,a)$

　　　　2. for i=1 to 最大迭代次数T：

　　　　　　a) S $$ current state

　　　　　　b) A $\gets$ $\epsilon-greedy(S,Q)$

　　　　　　c) 执行动作$A$,得到新状态$S'$和奖励$R$

　　　　　　d) 使用Q-Learning更新价值函数：$Q(S,A) =Q(S,A) + \alpha[R +\gamma\max_aQ(S',a) -Q(S,A)]$

　　　　　　e) 使用$S,A,S'$更新状态模型$P(s,a)$，使用$S,A,R$更新状态模型$R(s,a)$

　　　　　　f) for j=1 to 最大次数n：

　　　　　　　　i) 随机选择一个之前出现过的状态$S$, 在状态$S$上出现过的动作中随机选择一个动作$A$

　　　　　　　　ii) 基于模型$P(S,A)$得到$S'$, 基于模型$R(S,A)$得到$R$

　　　　　　　　iii) 使用Q-Learning更新价值函数：$Q(S,A) =Q(S,A) + \alpha[R +\gamma\max_aQ(S',a) -Q(S,A)]$

　　　　从上面的流程可以看出，Dyna框架在每个迭代轮中，会先和环境交互，并更新价值函数和（或）策略函数，接着进行n次模型的预测，同样更新价值函数和（或）策略函数。这样同时利用上了和环境交互的经历以及模型的预测。

## 5. Dyna-2算法框架

　　　　在Dyna算法框架的基础上后来又发展出了Dyna-2算法框架。和Dyna相比，Dyna-2将和和环境交互的经历以及模型的预测这两部分使用进行了分离。还是以Q函数为例，Dyna-2将记忆分为永久性记忆（permanent memory）和瞬时记忆（transient memory）, 其中永久性记忆利用实际的经验来更新，瞬时记忆利用模型模拟经验来更新。

　　　　永久性记忆的Q函数定义为：$$Q(S,A) = \phi(S,A)^T\theta$$

　　　　瞬时记忆的Q函数定义为：$$Q'(S,A) = \overline{\phi}(S,A)^T\overline{\theta }$$

　　　　组合起来后记忆的Q函数定义为：$$\overline{Q}(S,A) = \phi(S,A)^T\theta + \overline{\phi}(S,A)^T\overline{\theta }$$

　　　　Dyna-2的基本思想是在选择实际的执行动作前，智能体先执行一遍从当前状态开始的基于模型的模拟，该模拟将仿真完整的轨迹，以便评估当前的动作值函数。智能体会根据模拟得到的动作值函数加上实际经验得到的值函数共同选择实际要执行的动作。价值函数的更新方式类似于$SARSA(\lambda)$

　　　　以下是Dyna-2的算法流程：

![img](https://ask.qcloudimg.com/http-save/yehe-2770378/e3sfeyaflk.png?imageView2/2/w/1620)

## 6. 基于模型的强化学习总结

　　　　基于模型的强化学习一般不单独使用，而是和不基于模型的强化学习结合起来，因此使用Dyna算法框架是常用的做法。对于模型部分，我们可以用查表法和监督学习法等方法，预测或者采样得到模拟的经历。而对于非模型部分，使用前面的Q-Learning系列的价值函数近似，或者基于Actor-Critic的策略函数的近似都是可以的。

　　　　除了Dyna算法框架，我们还可以使用基于模拟的搜索(simulation-based search)来结合基于模型的强化学习和不基于模型的强化学习,并求解问题。这部分我们在后面再讨论。



# 强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)

发布于2019-03-15 16:29:27阅读 5650

　　　　在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。

　　　　本篇主要参考了UCL强化学习课程的第八讲，第九讲部分。

## 1. 基于模拟的搜索概述

　　　　什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。

　　　　那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点S_t开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以S_t为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到S_t状态最应该采用的动作A_t。前向搜索的sub-MDP如下图：

![img](https://ask.qcloudimg.com/http-save/yehe-2770378/jtwoxt67mq.png?imageView2/2/w/1620)

　　　　前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。

## 2. 简单蒙特卡罗搜索

　　　　首先我们看看基于模拟的搜索中比较简单的一种方法：简单蒙特卡罗搜索。

　　　　简单蒙特卡罗搜索基于一个强化学习模型$M_v$和一个模拟策略$\pi$.在此基础上，对于当前我们要选择动作的状态$S_t$, 对每一个可能采样的动作$a \in A$,都进行$K$轮采样，这样每个动作$a$都会得到K组经历完整的状态序列(episode)。即：
$$
\{S_t,a, R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,......S_T^k\}_{k=1}^K \sim M_v,\pi
$$
　　　　现在对于每个
$$
(S_t,a)
$$
组合，我们可以基于蒙特卡罗法来计算其动作价值函数并选择最优的动作了。
$$
Q(S_t,a) = \frac{1}{K}\sum\limits_{k=1}^KG_t
$$

$$
a_t =\arg\max_{a \in A}Q(S_t,a)
$$

 　　　简单蒙特卡罗搜索和起前向搜索比起来，对于状态动作数量的处理能力上了一个数量级,可以处理中等规模的问题。但是假如我们的状态动作数量达到非常大的量级，比如围棋的级别,那么简单蒙特卡罗搜索也太慢了。同时，由于使用蒙特卡罗法计算其动作价值函数，模拟采样得到的一些中间状态和对应行为的价值就被忽略了，这部分数据能不能利用起来呢？

 　　　下面我们看看蒙特卡罗树搜索(Monte-Carlo Tree Search，以下简称MCTS)怎么优化这个问题的解决方案。

## 3. MCTS的原理

　　　　MCTS**摒弃了**简单蒙特卡罗搜索里面对当前状态$S_t$每个动作都要进行K次模拟采样的做法，**而是**总共对当前状态$S_t$进行K次采样，这样**采样**到的动作只是动作全集A中的一部分。这样做大大降低了采样的数量和采样后的搜索计算。当然，代价是可能动作全集中的很多动作都没有采样到，可能错失好的动作选择，这是一个算法设计上的折衷。

　　　　在MCTS中，基于一个强化学习模型$M_v$和一个模拟策略$\pi$，当前状态$S_t$对应的完整的状态序列(episode)是这样的：
$$
\{S_t,A_t^k, R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,......S_T^k\}_{k=1}^K \sim M_v,\pi
$$
　　　　采样完毕后，我们可以基于采样的结果构建一颗MCTS的搜索树，然后近似计算$Q(s_t,a)$和最大$Q(s_t,a)$对应的动作。
$$
Q(S_t,a) = \frac{1}{K}\sum\limits_{k=1}^K\sum\limits_{u=t}^T1(S_{uk}=S_t, A_{uk} =a)G_u
$$
　　
$$
a_t =\arg\max_{a \in A}Q(S_t,a)
$$
　　　　MCTS搜索的策略分为两个阶段：第一个是树内策略(tree policy)：为当模拟采样得到的状态存在于当前的MCTS时使用的策略。树内策略可以使$\epsilon-$贪婪策略，随着模拟的进行策略可以得到持续改善，还可以使用上限置信区间算法UCT，这在棋类游戏中很普遍；第二个是默认策略(default policy)：如果当前状态不在MCTS内，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于目标价值函数的策略。

　　　　这里讲到的是最经典的强化学习终MCTS的用户，每一步都有延时奖励，但是在棋类之类的零和问题中，中间状态是没有明确奖励的，我们只有在棋下完后知道输赢了才能对前面的动作进行状态奖励，对于这类问题我们的MCTS需要做一些结构上的细化。

## 4. 上限置信区间算法UCT

　　　　在讨论棋类游戏的MCTS搜索之前，我们先熟悉下上限置信区间算法(Upper Confidence Bound Applied to Trees, 以下简称UCT)。它是一种策略算法，我们之前最常用的是
$$
\epsilon-贪婪策略。
$$
　　　　但是在棋类问题中，UCT更常使用。

在棋类游戏中，经常有这样的问题，我们发现在某种棋的状态下，有2个可选动作，第一个动作历史棋局中是0胜1负，第二个动作历史棋局中是8胜10负，那么我们应该选择哪个动作好呢？如果按$\epsilon-$贪婪策略，则第二个动作非常容易被选择到。但是其实虽然第一个动作胜利0%，但是很可能是因为这个动作的历史棋局少，数据不够导致的，很可能该动作也是一个不错的动作。那么我们如何在最优策略和探索度达到一个选择平衡呢？$\epsilon-$贪婪策略可以用，但是UCT是一个更不错的选择。

　　　　UCT首先计算每一个可选动作节点对应的分数，这个分数考虑了历史最优策略和探索度吗，一个常用的公式如下：
$$
\text{score = }\ \frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}
$$
　　　　其中，$w_i$ 是 i 节点的胜利次数，$n_i$ 是i节点的模拟次数，$N_i$是所有模拟次数，c 是探索常数，理论值为$\sqrt{2}$，可根据经验调整，c 越大就越偏向于广度搜索，c 越小就越偏向于深度搜索。最后我们选择分数最高的动作节点。

　　　　比如对于下面的棋局，对于根节点来说，有3个选择，第一个选择7胜3负，第二个选择5胜3负，第三个选择0胜3负。

　　　　如果我们取C=10,则第一个节点的分数为：
$$
score(7,10) =7/10 + C \cdot \sqrt{\frac{\log(21)}{10}} \approx 6.2 
$$
　　　　第二个节点的分数为：
$$
score(5,8) = 5/8 + C \cdot \sqrt{\frac{\log(21)}{8}} \approx 6.8 
$$
　　　　第三个节点的分数为：
$$
score(0,3) = 0/3 + C \cdot \sqrt{\frac{\log(21)}{3}} \approx 10
$$
![img](https://ask.qcloudimg.com/http-save/yehe-2770378/l0w9gidsiq.png?imageView2/2/w/1620)

　　　　可见，由于我们把探索率c设置的比较大，第三个节点是被UCT选中要执行的动作节点。当然如果我们把c设置的比较小的话，第一个或者第二个可能就变成最大的分数了。

## 5. 棋类游戏MCTS搜索

　　　　在像中国象棋，围棋这样的零和问题中，一个动作只有在棋局结束才能拿到真正的奖励，因此我们对MCTS的搜索步骤和树结构上需要根据问题的不同做一些细化。

　　　　对于MCTS的树结构，如果是最简单的方法，只需要在节点上保存状态对应的历史胜负记录。在每条边上保存采样的动作。这样MCTS的搜索需要走4步，如下图(图来自维基百科)：

![img](https://ask.qcloudimg.com/http-save/yehe-2770378/y3lyiqm46x.png?imageView2/2/w/1620)

　　　　第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。之所以叫做“存在未扩展的子节点”，是因为这个局面存在未走过的后续着法，也就是MCTS中没有后续的动作可以参考了。这时我们进入第二步。

　　　　第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个0/0的子节点，表示没有历史记录参考。这时我们进入第三步。

　　　　第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。

　　　　第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录，如上图最右边所示。

　　　　以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。

## 6. MCTS小结

　　　　MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。